# .env.example - Environment Configuration Template
# ⚠️ DO NOT COMMIT .env TO GIT - IT CONTAINS SENSITIVE INFORMATION!
# Copy this to .env and fill in your actual values

# ========== API KEYS ==========
# Leave empty if using local Ollama (recommended for privacy)
GEMINI_API_KEY=
OPENAI_API_KEY=

# ========== LLM BACKEND SELECTION ==========
# Options: gemini, ollama, groq, openai
LLM_BACKEND=ollama

# Gemini API settings (if using GEMINI backend)
GEMINI_MODEL=gemini-2.0-flash

# Ollama settings (local LLM - recommended)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=phi
# Available models: phi (2.7B - fastest), mistral (7B), llama2, llama3

# ========== EMBEDDING MODEL ==========
# Model for converting text to vectors
EMBEDDING_MODEL=all-mpnet-base-v2
# Alternatives: all-MiniLM-L6-v2 (faster, less accurate)
#              all-MiniLM-L12-v2 (balanced)

# Device for embeddings: cuda (GPU), cpu (CPU), mps (Apple Silicon)
EMBEDDING_DEVICE=cpu

# ========== VECTOR DATABASE ==========
VECTORDB_PATH=./chromadb
VECTORDB_TYPE=chromadb

# ========== RETRIEVAL PARAMETERS ==========
# Number of chunks to retrieve per query
RETRIEVAL_TOP_K=5
# Minimum similarity score threshold (0.0-1.0)
RETRIEVAL_SCORE_THRESHOLD=0.3

# ========== LOGGING ==========
LOG_LEVEL=INFO
LOG_FILE=./output/logs/app.log

# ========== PATHS ==========
DOCS_PATH=./docs
OUTPUT_PATH=./output
